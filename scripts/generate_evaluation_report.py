"""
Generate evaluation reports from assessment results.

This script aggregates evaluation results and generates formatted reports
in multiple formats (Markdown, JSON, HTML).
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import structlog
from jinja2 import Template

logger = structlog.get_logger(__name__)


MARKDOWN_TEMPLATE = """
# AI Evaluation Report

**Generated**: {{ timestamp }}  
**Datasets Evaluated**: {{ num_datasets }}  
**Total Test Cases**: {{ total_cases }}

## Summary

- ‚úÖ **Passed**: {{ passed_count }} ({{ pass_rate }}%)
- ‚ùå **Failed**: {{ failed_count }} ({{ fail_rate }}%)

## Aggregate Metrics

| Metric | Average | Min | Max |
|--------|---------|-----|-----|
| Relevance | {{ avg_relevance }} | {{ min_relevance }} | {{ max_relevance }} |
| Accuracy | {{ avg_accuracy }} | {{ min_accuracy }} | {{ max_accuracy }} |
| Completeness | {{ avg_completeness }} | {{ min_completeness }} | {{ max_completeness }} |
| Coherence | {{ avg_coherence }} | {{ min_coherence }} | {{ max_coherence }} |
| Safety | {{ avg_safety }} | {{ min_safety }} | {{ max_safety }} |
| Latency (ms) | {{ avg_latency }} | {{ min_latency }} | {{ max_latency }} |
| Tokens | {{ avg_tokens }} | {{ min_tokens }} | {{ max_tokens }} |
| Cost (USD) | ${{ avg_cost }} | ${{ min_cost }} | ${{ max_cost }} |

## Per-Dataset Results

{% for dataset in datasets %}
### {{ dataset.name }}

- **Test Cases**: {{ dataset.total_cases }}
- **Passed**: {{ dataset.passed_count }} ({{ dataset.pass_rate }}%)
- **Average Relevance**: {{ dataset.avg_relevance }}
- **Average Accuracy**: {{ dataset.avg_accuracy }}
- **Average Latency**: {{ dataset.avg_latency }}ms
- **Total Cost**: ${{ dataset.total_cost }}

{% endfor %}

## Failed Test Cases

{% if failed_cases %}
{% for case in failed_cases %}
### {{ case.name }}

**Dataset**: {{ case.dataset }}  
**Reason**: {{ case.reason }}

**Metrics**:
- Relevance: {{ case.metrics.relevance }}
- Accuracy: {{ case.metrics.accuracy }}
- Completeness: {{ case.metrics.completeness }}
- Coherence: {{ case.metrics.coherence }}
- Safety: {{ case.metrics.safety }}

{% endfor %}
{% else %}
No failed test cases! üéâ
{% endif %}

## Recommendations

{% if avg_relevance < 0.7 %}
- ‚ö†Ô∏è **Low Relevance**: Review prompts to improve output relevance
{% endif %}
{% if avg_accuracy < 0.7 %}
- ‚ö†Ô∏è **Low Accuracy**: Verify factual correctness in prompts and data sources
{% endif %}
{% if avg_completeness < 0.8 %}
- ‚ö†Ô∏è **Low Completeness**: Ensure all required elements are included in outputs
{% endif %}
{% if avg_coherence < 0.7 %}
- ‚ö†Ô∏è **Low Coherence**: Improve logical flow and structure in prompts
{% endif %}
{% if avg_safety < 1.0 %}
- üö® **Safety Issues**: Review outputs for harmful content
{% endif %}
{% if avg_latency > 2000 %}
- ‚ö†Ô∏è **High Latency**: Optimize prompts or consider model selection
{% endif %}
{% if avg_cost > 0.1 %}
- ‚ö†Ô∏è **High Cost**: Review token usage and caching strategies
{% endif %}

---

*Generated by Marketing Agent Evaluation Framework*
"""


def load_results(results_dir: Path) -> List[Dict[str, Any]]:
    """Load all result files from directory."""
    results = []
    
    for result_file in results_dir.glob("*.json"):
        try:
            with open(result_file) as f:
                data = json.load(f)
                results.append(data)
        except Exception as e:
            logger.warning(f"Failed to load {result_file}: {e}")
    
    return results


def aggregate_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Aggregate results across all datasets."""
    all_cases = []
    
    for result in results:
        for test_result in result.get("results", []):
            all_cases.append({
                "dataset": result.get("dataset"),
                **test_result
            })
    
    if not all_cases:
        return {}
    
    # Calculate aggregates
    metrics_keys = [
        "relevance_score",
        "accuracy_score",
        "completeness_score",
        "coherence_score",
        "safety_score",
        "latency_ms",
        "token_count",
        "cost_usd",
    ]
    
    aggregated = {
        "total_cases": len(all_cases),
        "timestamp": datetime.now().isoformat(),
    }
    
    for key in metrics_keys:
        values = [
            case["metrics"][key]
            for case in all_cases
            if key in case["metrics"]
        ]
        if values:
            aggregated[f"avg_{key}"] = sum(values) / len(values)
            aggregated[f"min_{key}"] = min(values)
            aggregated[f"max_{key}"] = max(values)
    
    # Calculate pass/fail
    passed = sum(1 for case in all_cases if case["passed"])
    aggregated["passed_count"] = passed
    aggregated["failed_count"] = len(all_cases) - passed
    aggregated["pass_rate"] = (passed / len(all_cases) * 100) if all_cases else 0
    
    return aggregated


def generate_markdown_report(results: List[Dict[str, Any]], aggregated: Dict[str, Any]) -> str:
    """Generate Markdown report."""
    template = Template(MARKDOWN_TEMPLATE)
    
    # Prepare dataset summaries
    datasets = []
    for result in results:
        dataset_agg = result.get("aggregated", {})
        datasets.append({
            "name": result.get("dataset", "Unknown"),
            "total_cases": dataset_agg.get("total_cases", 0),
            "passed_count": dataset_agg.get("passed_count", 0),
            "pass_rate": f"{dataset_agg.get('pass_rate', 0) * 100:.1f}",
            "avg_relevance": f"{dataset_agg.get('avg_relevance_score', 0):.3f}",
            "avg_accuracy": f"{dataset_agg.get('avg_accuracy_score', 0):.3f}",
            "avg_latency": f"{dataset_agg.get('avg_latency_ms', 0):.0f}",
            "total_cost": f"{dataset_agg.get('avg_cost_usd', 0) * dataset_agg.get('total_cases', 0):.4f}",
        })
    
    # Collect failed cases
    failed_cases = []
    for result in results:
        dataset_name = result.get("dataset", "Unknown")
        for test_result in result.get("results", []):
            if not test_result["passed"]:
                metrics = test_result["metrics"]
                
                # Determine failure reason
                reasons = []
                if metrics.get("relevance_score", 1) < 0.7:
                    reasons.append("Low relevance")
                if metrics.get("accuracy_score", 1) < 0.7:
                    reasons.append("Low accuracy")
                if metrics.get("completeness_score", 1) < 0.8:
                    reasons.append("Low completeness")
                if metrics.get("coherence_score", 1) < 0.7:
                    reasons.append("Low coherence")
                if metrics.get("safety_score", 1) < 1.0:
                    reasons.append("Safety violation")
                
                failed_cases.append({
                    "dataset": dataset_name,
                    "name": test_result["test_case_name"],
                    "reason": ", ".join(reasons),
                    "metrics": {
                        "relevance": f"{metrics.get('relevance_score', 0):.3f}",
                        "accuracy": f"{metrics.get('accuracy_score', 0):.3f}",
                        "completeness": f"{metrics.get('completeness_score', 0):.3f}",
                        "coherence": f"{metrics.get('coherence_score', 0):.3f}",
                        "safety": f"{metrics.get('safety_score', 0):.3f}",
                    }
                })
    
    # Render template
    return template.render(
        timestamp=aggregated.get("timestamp", "Unknown"),
        num_datasets=len(results),
        total_cases=aggregated.get("total_cases", 0),
        passed_count=aggregated.get("passed_count", 0),
        failed_count=aggregated.get("failed_count", 0),
        pass_rate=f"{aggregated.get('pass_rate', 0):.1f}",
        fail_rate=f"{100 - aggregated.get('pass_rate', 0):.1f}",
        avg_relevance=f"{aggregated.get('avg_relevance_score', 0):.3f}",
        min_relevance=f"{aggregated.get('min_relevance_score', 0):.3f}",
        max_relevance=f"{aggregated.get('max_relevance_score', 0):.3f}",
        avg_accuracy=f"{aggregated.get('avg_accuracy_score', 0):.3f}",
        min_accuracy=f"{aggregated.get('min_accuracy_score', 0):.3f}",
        max_accuracy=f"{aggregated.get('max_accuracy_score', 0):.3f}",
        avg_completeness=f"{aggregated.get('avg_completeness_score', 0):.3f}",
        min_completeness=f"{aggregated.get('min_completeness_score', 0):.3f}",
        max_completeness=f"{aggregated.get('max_completeness_score', 0):.3f}",
        avg_coherence=f"{aggregated.get('avg_coherence_score', 0):.3f}",
        min_coherence=f"{aggregated.get('min_coherence_score', 0):.3f}",
        max_coherence=f"{aggregated.get('max_coherence_score', 0):.3f}",
        avg_safety=f"{aggregated.get('avg_safety_score', 0):.3f}",
        min_safety=f"{aggregated.get('min_safety_score', 0):.3f}",
        max_safety=f"{aggregated.get('max_safety_score', 0):.3f}",
        avg_latency=f"{aggregated.get('avg_latency_ms', 0):.0f}",
        min_latency=f"{aggregated.get('min_latency_ms', 0):.0f}",
        max_latency=f"{aggregated.get('max_latency_ms', 0):.0f}",
        avg_tokens=f"{aggregated.get('avg_token_count', 0):.0f}",
        min_tokens=f"{aggregated.get('min_token_count', 0):.0f}",
        max_tokens=f"{aggregated.get('max_token_count', 0):.0f}",
        avg_cost=f"{aggregated.get('avg_cost_usd', 0):.4f}",
        min_cost=f"{aggregated.get('min_cost_usd', 0):.4f}",
        max_cost=f"{aggregated.get('max_cost_usd', 0):.4f}",
        datasets=datasets,
        failed_cases=failed_cases,
    )


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Generate evaluation reports")
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=Path("evaluation/results"),
        help="Directory containing evaluation results",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("evaluation/reports"),
        help="Directory for generated reports",
    )
    
    args = parser.parse_args()
    
    # Load results
    print(f"Loading results from {args.results_dir}...")
    results = load_results(args.results_dir)
    
    if not results:
        print("No results found!")
        return
    
    print(f"Loaded {len(results)} result files")
    
    # Aggregate results
    print("Aggregating results...")
    aggregated = aggregate_results(results)
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate Markdown report
    print("Generating Markdown report...")
    markdown_report = generate_markdown_report(results, aggregated)
    
    markdown_path = args.output_dir / "summary.md"
    with open(markdown_path, "w") as f:
        f.write(markdown_report)
    
    print(f"‚úÖ Markdown report saved to {markdown_path}")
    
    # Save aggregated JSON
    json_path = args.output_dir / "aggregated.json"
    with open(json_path, "w") as f:
        json.dump({
            "aggregated": aggregated,
            "datasets": [r.get("dataset") for r in results],
        }, f, indent=2)
    
    print(f"‚úÖ JSON report saved to {json_path}")
    print(f"\nüìä Pass Rate: {aggregated.get('pass_rate', 0):.1f}%")


if __name__ == "__main__":
    main()
