# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI EVALUATION WORKFLOW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# PURPOSE:
# This workflow tests the quality of AI agent outputs using automated evaluation.
# It runs test datasets through the AI agent and measures performance metrics.
#
# WHY THIS MATTERS FOR AI SYSTEMS:
# Unlike traditional software (where tests either pass/fail), AI outputs are
# probabilistic and vary each run. We need continuous evaluation to:
# - Detect regressions (did prompt changes make outputs worse?)
# - Measure quality metrics (relevance, accuracy, coherence)
# - Ensure safety (prevent harmful/biased outputs)
# - Track performance over time
#
# WHAT IT EVALUATES:
# - Campaign optimization agent decisions
# - Creative performance analysis quality
# - Response relevance, accuracy, completeness
# - Safety guardrails (no inappropriate content)
#
# WHEN IT RUNS:
# - On pull requests (before merging changes to prompts/agent code)
# - On push to main (after merging, validate production readiness)
# - Daily at 2 AM UTC (catch model drift or external API changes)
# - Manually (for ad-hoc testing with specific datasets)
#
# LEARNING - AI EVALUATION CONCEPTS:
# - Golden Dataset: Curated test cases with known-good outputs
# - Pass Rate: % of test cases that meet quality thresholds
# - LangSmith: Observability platform for tracing LLM calls
# - Regression: When AI performs worse than baseline (previous version)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

name: AI Evaluation

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WORKFLOW TRIGGERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
on:
  # Trigger on PRs that change agent code, prompts, or evaluation datasets
  # Path filters save compute - only run when relevant files change
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/**'                    # Backend Python code (agent logic)
      - 'prompts/**'                # LLM prompt templates
      - 'evaluation/datasets/**'    # Test datasets
  
  # Run on main branch pushes (validate production code)
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'prompts/**'
  
  # Daily scheduled run - detect model drift or API changes
  # Model providers (OpenAI, Anthropic) update models frequently
  schedule:
    - cron: '0 2 * * *'   # 2 AM UTC daily
  
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to evaluate (or "all" for all datasets)'
        required: false
        default: 'all'
      enable_tracing:
        description: 'Enable LangSmith tracing'  # Tracing helps debug failures
        required: false
        default: 'true'

# Global environment variables for all jobs
env:
  PYTHON_VERSION: '3.11'    # Consistent Python version

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# JOBS: AI EVALUATION PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

jobs:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # JOB 1: RUN AI EVALUATION
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # WHAT: Runs golden datasets through the AI agent and evaluates outputs
  # HOW: Uses evaluation scripts with predefined test cases and scoring criteria
  # OUTPUT: Pass rates, quality scores, detailed reports
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  evaluate:
    name: Run AI Evaluation
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Get code
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,evaluation]"
      
      # Step 4: Create directory for evaluation results
      - name: Create results directory
        run: mkdir -p evaluation/results
      
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # EVALUATION DATASET 1: CAMPAIGN OPTIMIZATION
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # Tests: AI agent's ability to optimize marketing campaigns
      # Dataset: Scenario: Low CTR campaign, need optimization suggestions
      # Expected: Actionable recommendations (improve headlines, A/B test, etc.)
      # Metrics: Relevance, accuracy, actionability
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run evaluation on campaign optimization dataset
        env:
          # LangSmith: Tracing/logging platform for LLM applications
          # Traces every LLM call with inputs, outputs, latency, cost
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_TRACING_V2: ${{ github.event.inputs.enable_tracing || 'true' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}  # For GPT-4 calls
        run: |
          python scripts/run_evaluation.py campaign_optimization \
            --output-dir evaluation/results
      
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # EVALUATION DATASET 2: CREATIVE PERFORMANCE
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # Tests: AI agent's ability to analyze ad creative performance
      # Dataset: Ad metrics (impressions, clicks, conversions)
      # Expected: Insights on what's working/not working
      # Metrics: Insight quality, data interpretation accuracy
      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run evaluation on creative performance dataset
        env:
          LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
          LANGCHAIN_TRACING_V2: ${{ github.event.inputs.enable_tracing || 'true' }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/run_evaluation.py creative_performance \
            --output-dir evaluation/results
      
      # Step 5: Generate human-readable evaluation report
      # Aggregates results from all datasets into summary report
      # Output: Markdown report with pass/fail, scores, examples
      - name: Generate evaluation report
        run: |
          python scripts/generate_evaluation_report.py \
            --results-dir evaluation/results \
            --output-dir evaluation/reports
      
      # Step 6: Save detailed results (JSON with all test cases)
      # retention-days: How long GitHub keeps the artifact
      # (30 days balances storage costs with debuggability)
      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results-${{ github.sha }}  # Unique name per commit
          path: evaluation/results/
          retention-days: 30
      
      # Step 7: Save human-readable reports
      - name: Upload evaluation reports
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports-${{ github.sha }}
          path: evaluation/reports/
          retention-days: 30
      
      # Step 8: Check if results meet minimum quality thresholds
      # QUALITY GATES: Fail workflow if scores drop below thresholds
      # - Pass rate: 85%+ (at least 85% of test cases must pass)
      # - Relevance: 0.70+ (responses must be on-topic)
      # - Accuracy: 0.70+ (factual correctness in responses)
      # - Completeness: 0.80+ (all required info provided)
      # - Coherence: 0.70+ (well-structured, clear responses)
      # - Safety: 1.00 (ZERO tolerance for unsafe content)
      #
      # If thresholds are not met, workflow fails and PR is blocked
      - name: Check evaluation thresholds
        run: |
          python scripts/check_evaluation_thresholds.py \
            --results-dir evaluation/results \
            --min-pass-rate 0.85 \
            --min-relevance 0.70 \
            --min-accuracy 0.70 \
            --min-completeness 0.80 \
            --min-coherence 0.70 \
            --min-safety 1.00
      
      # Step 9: Post results as PR comment (only on pull requests)
      # Uses GitHub API to comment evaluation summary directly on PR
      # Makes results visible without needing to download artifacts
      - name: Comment PR with evaluation results
        if: github.event_name == 'pull_request'  # Only run on PRs
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'evaluation/reports/summary.md';
            
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,  # PR number
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ğŸ¤– AI Evaluation Results\n\n${report}`
              });
            }
  
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # JOB 2: PERFORMANCE BENCHMARKING
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # WHAT: Measures AI agent performance (latency, token usage, cost)
  # WHY: Track performance regressions and optimize for production
  # METRICS: Response time, tokens per request, cost per 1000 queries
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    needs: evaluate  # Run only after evaluation (â‡¨ wait for quality check first)
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,evaluation]"
      
      - name: Run performance benchmarks
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python scripts/run_benchmarks.py \
            --output-dir evaluation/benchmarks \
            --iterations 10
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: evaluation/benchmarks/
          retention-days: 30
      
      - name: Compare with baseline
        run: |
          python scripts/compare_benchmarks.py \
            --current evaluation/benchmarks/latest.json \
            --baseline evaluation/benchmarks/baseline.json \
            --max-regression 0.10
  
  regression-check:
    name: Check for Regressions
    runs-on: ubuntu-latest
    needs: [evaluate, benchmark]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,evaluation]"
      
      - name: Download evaluation results
        uses: actions/download-artifact@v4
        with:
          name: evaluation-results-${{ github.sha }}
          path: evaluation/results/current
      
      - name: Get base branch results
        run: |
          git checkout origin/${{ github.base_ref }}
          mkdir -p evaluation/results/baseline
          # Copy or retrieve baseline results
          # This would typically pull from a stored location
      
      - name: Compare evaluation results
        run: |
          python scripts/compare_evaluations.py \
            --current evaluation/results/current \
            --baseline evaluation/results/baseline \
            --output evaluation/reports/regression-analysis.md
      
      - name: Comment regression analysis
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'evaluation/reports/regression-analysis.md';
            
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ğŸ“Š Regression Analysis\n\n${report}`
              });
            }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END OF AI EVALUATION WORKFLOW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# SUMMARY - THE AI EVALUATION PIPELINE:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. EVALUATE Job â†’ Runs golden datasets, measures quality scores
# 2. BENCHMARK Job â†’ Measures performance (latency, tokens, cost)
# 3. REGRESSION CHECK Job â†’ Compares PR vs main branch (detect degradation)
#
# KEY CONCEPTS:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â€¢ Golden Dataset: Curated test cases representing real-world scenarios
# â€¢ Quality Metrics: Relevance, accuracy, completeness, coherence, safety
# â€¢ LangSmith Tracing: Records every LLM call for debugging
# â€¢ Threshold Gates: Workflow fails if quality/performance drops
# â€¢ Regression Analysis: Prevents bad changes from reaching production
#
# HOW TO USE:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Make changes to agent code or prompts
# 2. Create pull request
# 3. Evaluation runs automatically
# 4. Review results in PR comment
# 5. If failed, improve code and push again
# 6. Once passing, safe to merge!
#
# TROUBLESHOOTING:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â€¢ Evaluation failed? Check LangSmith traces in logs
# â€¢ Quality scores dropped? Review prompt changes, test edge cases
# â€¢ Performance regressed? Profile code, optimize prompt length
# â€¢ Safety violations? Review content filters, add guardrails
#
# BEST PRACTICES:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ“ Keep golden datasets up-to-date with real user queries
# âœ“ Set reasonable thresholds (too strict â†’ blocks valid changes)
# âœ“ Monitor pass rates over time (trending down = systemic issues)
# âœ“ Use LangSmith tracing to debug individual failures
# âœ“ Test edge cases (empty inputs, very long inputs, non-English)
# âœ“ Include adversarial examples (prompt injection attempts)
# âœ“ Document why specific thresholds were chosen
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
